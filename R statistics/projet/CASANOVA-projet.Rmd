---
title: "Fouille de données massives - Projet final"
author: "CASANOVA MARROQUIN Stephanya"
date: "2/16/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("FactoMineR")
library("factoextra")
library ("dplyr")
library("rpart")
library("rpart.plot")
library("rattle")
library("RColorBrewer")
library("randomForest")
```
***
### **Données  ** 

Le projet a été basé sur une base de données décrivant **6224 individus américains** décrits par 15 variables : 

* **Age** : âge  
* **CSP** : catégorie socio-professionnelle  
* **ScoreDemo** : un score démographique  
* **Diplome** : le type de diplôme  
* **ScoreDiplome** : un score construit en fonction du type de diplôme  
* **StatutMarital** : le statut marital  
* **Profession** : la profession  
* **SituationFamiliale** : la situation familiale  
* **Ethnie** : l’origine ethnique  
* **Genre** : le genre  
* **Economies** : le montant des économies  
* **Dettes** : le montant des dettes  
* **HeureSemaine** : le nombre d’heures travaillées par semaine  
* **PaysOrigine** : le pays d’origine    
* **Revenus** : montant des revenus (supérieur ou inférieur à 50k$)  

### **Import des données dans notre environnement  **  
  Pour charger les données dans notre environnement, nous allons utiliser le commande :   
``` {r}
data = read.table("adult_sample.data",sep=",",header=TRUE,strip.white=TRUE,na.strings ="?", stringsAsFactors = TRUE)
```
Dans ce command nous pouvons observer les parameters **sep** : réferénce au separateur, **strip.white** : supprime les espaces en blanc après d'un séparateur, **na.strings**: sustitution d'un string pour NA et **stringsAsFactors**.

***

### **Travail effectué**  
##### **1. Identification des valeurs nulls et imputation des valeurs**  
Apres d'avoir importé les données, nous allons identifier les valeur nulls et nous allons les imputer un valeur par defaut. Initialment, nous identifions les variables categorielles et quantitatives :
```{r ind_categ_feature}
ind_categ_feature = c(2,4,6,7,8,9,10,14,15)
data_categ = data[, ind_categ_feature]
data_continuous = data [,-ind_categ_feature]
```

Avec le command **summary()** nous allons regarder les details des données :   
```{r data_categ}
summary(data_categ)
```  

Dans cette information, nous pouvons observer que les variables **CSP, Profession et PaysOrigine** ont des valeurs **nulls**. 
```{r attWithNA}
attWithNA<-subset(data_categ, select=c(CSP, Profession, PaysOrigine))
summary(attWithNA)
```  

Ces valeurs nulls vont etre remplacés par de valeurs aletorios en fonction de la distribution de frequences dans chaque attribut : 

```{r naCSP}
naCSP=length(data_categ$CSP[is.na(data_categ$CSP)])
data_categ$CSP[is.na(data_categ$CSP)]=sample(levels(data_categ$CSP),naCSP,
prob=table(data_categ$CSP),replace=TRUE)
data_categ$CSP=as.factor(data_categ$CSP)
 
nbNA=length(data_categ$Profession[is.na(data_categ$Profession)])
data_categ$Profession[is.na(data_categ$Profession)]=sample(levels(data_categ$Profession),nbNA,
prob=table(data_categ$Profession),replace=TRUE)
data_categ$Profession=as.factor(data_categ$Profession)

nbNA=length(data_categ$PaysOrigine[is.na(data_categ$PaysOrigine)])
data_categ$PaysOrigine[is.na(data_categ$PaysOrigine)]=sample(levels(data_categ$PaysOrigine),nbNA,
prob=table(data_categ$PaysOrigine),replace=TRUE)
data_categ$PaysOrigine=as.factor(data_categ$PaysOrigine)
``` 

summary pour valider que effectivement les donnés ont été replacés :   

```{r attWithNA1}
attWithNA<-subset(data_categ, select=c(CSP, Profession, PaysOrigine))
summary(attWithNA)
```  

Maintenant, nous allons observer les details des attributes quantitatives :   

```{r summ}
summary(data_continuous)
```  

Dans les resultat, nous ne trouvons pas des valeurs nulls.

Nous allons supprimer le variable redondante ScoreDiplome puisque est le recodage de la variable Diplome :   
```{r recodage}
data_continuous$ScoreDiplome = NULL
```

##### **2. Analyse Factorielle**    

Maintenant avec la data traitée, nous allons faire un analyse en composant principales sur les variables continues :    
```{r acp}
res.pca <- PCA(data_continuous, graph = FALSE)
fviz_eig(res.pca, addlabels = TRUE)
```  
  
Du graphique ci-dessus, nous observons que le 82.9% (> 72%) des informations (variances) contenues dans les données sont conservées par les quatre premières composantes principales. 

```{r plot dim princ vs ortog}
# Colorer en fonction du cos2: qualité de représentation
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Évite le chevauchement de texte
             )
```    
  
Le graphique de corrélation des variables ci-dessus montre les relations entre toutes les variables. Il peut être interprété comme suit:     
* Les variables : **Dettes et HeureSemaine** et **Age et Economies** sont positivement corrélées.  
* Les variables : **ScoreDemo et (Age, Economies)** sont négativement corrélées.  
* Les variables **ScoreDemo, Age et HeureSemaine** ne sont pas bien répresentés pour l'ACP (cos2) à difference de **Dettes et Economies**  

Maintenant, nous allons regarder la contribution de chaque variable aux axes principales :      
```{r cont axe}
res.pca$var$contrib
```    

Maintenant, pour les individues :
```{r plot ind}
fviz_pca_ind(res.pca)
```   
  
Le graphique ci-dessus permet observer le comportement de chaque individue en considerant les variables : **age, economies, heureSemaine, dettes et scoreDemo**. Nous pouvons observer alors les gens ayent le plus de dettes et les plus d'heures travaillées en haut et les gens qui ont plus des economies et d'age en bas. Le groupe en bas à droite répresent un groupe avec des economies et des revenus elevés. Ces individues vont etre mis de côté pour l'analyse.  

```{r plot filter data}
data_categ$Economies = data_continuous$Economies
data_continuous<-filter(data_continuous, Economies != 99999)
data_categ<-filter(data_categ, Economies != 99999)
data_categ$Economies = NULL
res.pca <- PCA(data_continuous, graph = FALSE)
fviz_eig(res.pca, addlabels = TRUE)
```
```{r plot var economies filter}
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Évite le chevauchement de texte
             )
```


```{r plot ind economies filter}
fviz_pca_ind(res.pca)
```   

##### **3. Clustering**  
Comme les unités de mesure sont très différentes, nous allons normaliser les données :   
```{r plot normaliser}
data_continuous2=scale(data_continuous)
```   

Maintenant, nous allons utiliser l’inertie intra-classe pour choisir un nombre de clusters adequat :  
  
```{r plot clustering nb}
inertie=c()
for (k in 1:10){
res=kmeans(data_continuous2,centers=k,nstart = 10)
inertie[k]=res$tot.withinss
}
plot(1:10,inertie,type='l')
```

Selon le graphique, nous allons choisir 4 clusters.  

```{r plot clustering 4}
res_ka=kmeans(data_continuous2,centers=4,nstart = 10)
res.pca <- PCA(data_continuous2, graph = FALSE)
plot(res.pca,choix = "ind",col.ind = res_ka$cluster,graph.type = "classic")
```  
    
Ces 4 clusters pourraient correspondre à :     
* **Cluster axe 1 centre ** = Les gens qui travaillent le plus  
* **Cluster axe 1 en haut** = Les gens avec plus de dettes  
* **Cluster axe 2 centre ** = Les gens avec le scoreDemo plus haut  
* **Cluster axe 4** = Les gens avec plus des economies et agés  

```{r plot var clustering}
plot(res.pca,choix = "var",graph.type = 'classic')
```   

##### **4. Prediction ** 

##### **4.1 Ensamble d'entrainement et de testing  **    
Pour la base de test, nous allons prendre 1000 individus au hasard :  
```{r set test train}
set.seed(1)
ind_test=sample(1:nrow(data_continuous),1000)
dco_test=data_continuous[ind_test,]
dco_app=data_continuous[-ind_test,]
dca_test=data_categ[ind_test,]
dca_app=data_categ[-ind_test,]
```  

##### **4.2 Arbre de clasification  ** 
Pour la classification, nous allons utiliser un arbre binaire sur les données quantitatives. Il faut alors utiliser un dataframe et rapatrier la variable à prédire **Revenus** dans le même dataframe que les variables quantitatives :   

```{r plot var tree}
dfco_app=data.frame(dco_app,Revenus=dca_app$Revenus)
mod2=rpart(Revenus~.,data=dfco_app)
p=predict(mod2,newdata = dco_test,type='class')
fancyRpartPlot(mod2,caption=NULL)
```   

Pour évaluer les performances de l'arbre, nous allons observer la taux de bon classement et la matrice de confusion : 
```{r plot var perf1}
mean(p==dca_test$Revenus)
table(p, dca_test$Revenus)
```

On constate que la qualité de la prédiction dépend beaucoup de la classe. En effet, sur les 934 individuos qui ont revenus <= 50K, le taux de prévisions correctes est de 82.44 % environ, alors que sur les 66 individuos avec >50K, il n'est que de 92.42 % environ.  

Maintenant, si nous voudrions considereR tous les variables quantitatives et categorielles :  

```{r plot var tree2}
app=cbind(dco_app,dca_app)
test=cbind(dco_test,dca_test)
mod3=rpart(Revenus~.,data=app)
p=predict(mod3,newdata = test,type='class')
fancyRpartPlot(mod3,caption=NULL)
```  

Taux de bon classement et la matrice de confusion :  
```{r plot var perf 2}
mean(p==test$Revenus)
table(p, test$Revenus)
```

### **4.3 Forêt aléatoire  **  
Réaliser une classification avec une forêt aléatoire. L’hyper-paramètre mtry sera réglé de façon intelligente, sans
utiliser l’échantillon test. Evaluer la qualité de la prédiction sur les données tests et interpréter l’impact des différentes
variables dans la prédiction.

```{r plot foret}
mod4=randomForest(Revenus~.,data=app)
p=predict(mod4,newdata = test,type='class')
table(p, test$Revenus)
mean(p==test$Revenus)
varImpPlot(mod4)
```




