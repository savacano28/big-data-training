{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Stephanya CASANOVA MARROQUIN\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dac3401f7e517b1cec72732f9437fb58",
     "grade": false,
     "grade_id": "cell-dd54ad0671f620b9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Machine Learning in Spark\n",
    "\n",
    "Following the evolution of Spark, there are two ways to do Machine Learning on Spark :\n",
    "\n",
    "* MLlib, or `spark.mllib`, was the first ML library implemented in the core Spark library and runs on RDDs. As of today, the library is in maintenance mode, but as we did for RDDs vs DataFrames, it is important that we cover some aspects of the older library. MLlib is also the only library that supports training models for Spark Streaming. \n",
    "* ML, or `spark.ml` is now the primary ML library on Spark, and runs on DataFrames. Its API is close to those of other mainstream librairies like scikit-learn.\n",
    "\n",
    "We will dive into both APIs in this notebook, using the `titanic.csv` file for classification purposes on the `Survived` column.\n",
    "\n",
    "_I think at this point of your career, you all know what the [Titanic dataset](https://www.kaggle.com/c/titanic/data) is..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ba80560f7c7b1b281d9ab2ccdbe4b76",
     "grade": false,
     "grade_id": "cell-1e94907f4239a99c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.49:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lecture-lyon2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6c97344518>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName('lecture-lyon2').setMaster('local')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d56e90c8e0a2f67a92b3eaaf5feef3c",
     "grade": false,
     "grade_id": "cell-4374dd3b71e3e1aa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg import VectorUDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "527fbbddcd0832714c2618aa70ee0cbd",
     "grade": false,
     "grade_id": "cell-d28fe8378ff439c1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Data preparation\n",
    "\n",
    "Even though MLlib is designed with RDDs and DStreams in focus, for ease of transforming the data we will read the data and convert it to a DataFrame. Afterwards we will build RDDs for training in MLlib, or stay in DataFrame for training in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filePath = 'titanic.csv'\n",
    "data = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(filePath)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f6b121e284c520dc982f47aa2a64dab",
     "grade": false,
     "grade_id": "cell-ca71837cedb092f5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>714</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>446.0</td>\n",
       "      <td>0.3838383838383838</td>\n",
       "      <td>2.308641975308642</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>29.69911764705882</td>\n",
       "      <td>0.5230078563411896</td>\n",
       "      <td>0.38159371492704824</td>\n",
       "      <td>260318.54916792738</td>\n",
       "      <td>32.2042079685746</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>257.3538420152301</td>\n",
       "      <td>0.48659245426485753</td>\n",
       "      <td>0.8360712409770491</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14.526497332334035</td>\n",
       "      <td>1.1027434322934315</td>\n",
       "      <td>0.8060572211299488</td>\n",
       "      <td>471609.26868834975</td>\n",
       "      <td>49.69342859718089</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"</td>\n",
       "      <td>female</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A10</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>891</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>WE/P 5735</td>\n",
       "      <td>512.3292</td>\n",
       "      <td>T</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary        PassengerId             Survived              Pclass  \\\n",
       "0   count                891                  891                 891   \n",
       "1    mean              446.0   0.3838383838383838   2.308641975308642   \n",
       "2  stddev  257.3538420152301  0.48659245426485753  0.8360712409770491   \n",
       "3     min                  1                    0                   1   \n",
       "4     max                891                    1                   3   \n",
       "\n",
       "                                               Name     Sex  \\\n",
       "0                                               891     891   \n",
       "1                                              None    None   \n",
       "2                                              None    None   \n",
       "3  \"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"  female   \n",
       "4                       van Melkebeke, Mr. Philemon    male   \n",
       "\n",
       "                  Age               SibSp                Parch  \\\n",
       "0                 714                 891                  891   \n",
       "1   29.69911764705882  0.5230078563411896  0.38159371492704824   \n",
       "2  14.526497332334035  1.1027434322934315   0.8060572211299488   \n",
       "3                0.42                   0                    0   \n",
       "4                80.0                   8                    6   \n",
       "\n",
       "               Ticket               Fare Cabin Embarked  \n",
       "0                 891                891   204      889  \n",
       "1  260318.54916792738   32.2042079685746  None     None  \n",
       "2  471609.26868834975  49.69342859718089  None     None  \n",
       "3              110152                0.0   A10        C  \n",
       "4           WE/P 5735           512.3292     T        S  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27af6323e3cbecc226fa71cdd931a8ea",
     "grade": false,
     "grade_id": "cell-1ffea3dfd43c7ce6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "From the first summary statistics, we see that the `Age`, `Cabin` and `Embarked` variables can have null values. Also `PassengerId` and `Ticket` look useless for future predictions.\n",
    "\n",
    "# Question\n",
    "  \n",
    "* Drop `Cabin`, `Ticket` and `PassengerId`\n",
    "* Using `.na.fill` function on a DataFrame :\n",
    "    * For `Age`, replace `None` by the mean value for the column. \n",
    "    * For `Embarked` columns, replace `None` by the most frequent value for the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=data.drop('PassengerId','Ticket','Cabin')\n",
    "#mean = data.agg({\"Age\":\"avg\"}).collect()[0][\"avg(Age)\"]\n",
    "#freq = \"S\"\n",
    "#data=data.na.fill(freq,subset=[\"Embarked\"])\n",
    "#data=data.na.fill(mean,subset=[\"Age\"])\n",
    "#data.show()\n",
    "#data.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8981ccf6f3077abd1ad00725d9eb21db",
     "grade": false,
     "grade_id": "cell-852a22563e2c66a2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def replace_na(df):\n",
    "    # YOUR CODE HERE\n",
    "    data = df\n",
    "    data=data.drop('PassengerId','Ticket','Cabin')\n",
    "    mean = data.agg({\"Age\":\"avg\"}).collect()[0][\"avg(Age)\"]\n",
    "    freq = \"S\"\n",
    "    data=data.na.fill(freq,subset=[\"Embarked\"])\n",
    "    data=data.na.fill(mean,subset=[\"Age\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "495eb4f93d97689053c64d07cc38cc68",
     "grade": true,
     "grade_id": "cell-9c65ef235a646501",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graded cell\n",
    "\n",
    "3 points\n",
    "\"\"\"\n",
    "result = replace_na(data)\n",
    "assert float(result.describe().toPandas().loc[2]['Age']) - 13 < 0.1\n",
    "assert int(result.describe().toPandas().loc[0]['Embarked']) == 891\n",
    "assert list(result.toPandas().columns.values) == ['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1de211c78b3973a7f5d921c6b072d06a",
     "grade": false,
     "grade_id": "cell-c720dda0d25f41c5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For the following two questions, we will use [Transformers](https://spark.apache.org/docs/2.2.0/ml-pipeline.html#transformers). Technically, a Transformer implements a method `transform()`, which converts one DataFrame into another, generally by appending one or more columns.\n",
    "\n",
    "Example: \n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 0.1),\n",
    "    (1, 0.8),\n",
    "    (2, 0.2)\n",
    "], [\"id\", \"feature\"])\n",
    "continuousDataFrame.show()\n",
    "\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
    "binarizedDataFrame.show()\n",
    "```\n",
    "\n",
    "Result :\n",
    "\n",
    "```\n",
    "+---+-------+\n",
    "| id|feature|\n",
    "+---+-------+\n",
    "|  0|    0.1|\n",
    "|  1|    0.8|\n",
    "|  2|    0.2|\n",
    "+---+-------+\n",
    "\n",
    "Binarizer output with Threshold = 0.500000\n",
    "+---+-------+-----------------+\n",
    "| id|feature|binarized_feature|\n",
    "+---+-------+-----------------+\n",
    "|  0|    0.1|              0.0|\n",
    "|  1|    0.8|              1.0|\n",
    "|  2|    0.2|              0.0|\n",
    "+---+-------+-----------------+\n",
    "```\n",
    "\n",
    "**Note:** contrary to previous notebooks, I have not imported all of the libraries needed to solve the remaining exercises. When you want to import a library, please import it in the same notebook cell as where you implement your code, otherwise it may impact the automatic grading.\n",
    "\n",
    "# Question\n",
    "\n",
    "Through some regex, the [regex_extract UDF](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF) and [SQLTransformer](https://spark.apache.org/docs/2.2.0/ml-features.html#sqltransformer), get the title of a person from the `Name` column in a `Title` column. Drop the `Name` column afterwards.\n",
    "\n",
    "Example\n",
    "\n",
    "```\n",
    "Braund, Mr. Owen       --> Mr\n",
    "Andria, Doctor. Steve  --> Doctor\n",
    "```\n",
    "\n",
    "_Not a hint: while it is perfectly possible to write a custom UDF to solve this question, it breaks the purpose of using Dataframes for cleaning because UDFs don't benefit from SparkSQL's optimizer engine and have to transform back to Java objects for processing. Spark built-in UDFs don't share this problem._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.toPandas()[\"Name\"]\n",
    "#type()\n",
    "#from pyspark.sql.functions import regexp_extract, col\n",
    "#result = data.withColumn('Civility', regexp_extract(col('Name'), ', (.+?)\\. ',1))\n",
    "#list(result.select('Civility').distinct().toPandas()['Civility'].sort_values().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f63d00d042b0e75410b2cb613b62c616",
     "grade": false,
     "grade_id": "cell-93a47dbea9af3ed9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_civility(df):    \n",
    "    \"\"\"\n",
    "    Return dataframe dropping Name and replacing with Title\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    from pyspark.sql.functions import regexp_extract, col\n",
    "    result = data.withColumn('Civility', regexp_extract(col('Name'), ', (.+?)\\. ',1))\n",
    "    result = result.drop(\"Name\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaf4120d9637141e40ac9327d2061d52",
     "grade": true,
     "grade_id": "cell-c98d26534a8b160c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graded cell\n",
    "\n",
    "4 points\n",
    "\"\"\"\n",
    "result = extract_civility(data)\n",
    "resultCols = result.columns\n",
    "assert 'Name' not in resultCols\n",
    "assert 'Civility' in resultCols\n",
    "assert list(result.select('Civility').distinct().toPandas()['Civility'].sort_values().values) == [\n",
    "    'Capt',\n",
    "    'Col',\n",
    "    'Don',\n",
    "    'Dr',\n",
    "    'Jonkheer',\n",
    "    'Lady',\n",
    "    'Major',\n",
    "    'Master',\n",
    "    'Miss',\n",
    "    'Mlle',\n",
    "    'Mme',\n",
    "    'Mr',\n",
    "    'Mrs',\n",
    "    'Ms',\n",
    "    'Rev',\n",
    "    'Sir',\n",
    "    'the Countess'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "840b8c2bfea63a5ef297d22ad9a756a8",
     "grade": false,
     "grade_id": "cell-263e03090b6d1030",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Question \n",
    "\n",
    "[One hot encode](https://spark.apache.org/docs/2.2.0/ml-features.html#onehotencoder) `Sex`, `Civility` and `Embarked` columns into `SexVec`, `CivilityVec` and `EmbarkedVec`. Don't forget to drop the original columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6fc89f3c0a93f77ce190d1f08ccd505",
     "grade": false,
     "grade_id": "cell-87b6e00fa578a061",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df):\n",
    "    \"\"\"\n",
    "    Return dataframe one hot encoding selected columns    \n",
    "    \"\"\"\n",
    "    from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "    \n",
    "    stringSexIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "    modelSex = stringSexIndexer.setHandleInvalid(\"skip\").fit(df)\n",
    "    indexedSex = modelSex.transform(df)\n",
    "    encoderSex = OneHotEncoder(inputCol=\"SexIndex\", outputCol=\"SexVec\")\n",
    "    df = encoderSex.transform(indexedSex)\n",
    "\n",
    "    stringCivilityIndexer = StringIndexer(inputCol=\"Civility\", outputCol=\"CivilityIndex\")\n",
    "    modelCivility = stringCivilityIndexer.setHandleInvalid(\"skip\").fit(df)\n",
    "    indexedCivility = modelCivility.transform(df)\n",
    "    encoderCivility = OneHotEncoder(inputCol=\"CivilityIndex\", outputCol=\"CivilityVec\")\n",
    "    df = encoderCivility.transform(indexedCivility)\n",
    "\n",
    "    stringEmbarkedIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")\n",
    "    modelEmbarked = stringEmbarkedIndexer.setHandleInvalid(\"skip\").fit(df)\n",
    "    indexedEmbarked = modelEmbarked.transform(df)\n",
    "    encoderEmbarked = OneHotEncoder(inputCol=\"EmbarkedIndex\", outputCol=\"EmbarkedVec\")\n",
    "    df = encoderEmbarked.transform(indexedEmbarked)\n",
    "\n",
    "    df = df.drop('Sex','SexIndex','Civility','CivilityIndex','Embarked','EmbarkedIndex')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1700e84915682868d08698e63761e2e",
     "grade": true,
     "grade_id": "cell-2ca43cb570eae17b",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graded cell\n",
    "\n",
    "4 points\n",
    "\"\"\"\n",
    "result = one_hot_encode(extract_civility(data))\n",
    "resultCols = result.columns\n",
    "assert len(resultCols) == 12\n",
    "\n",
    "assert 'SexVec' in resultCols\n",
    "assert 'CivilityVec' in resultCols\n",
    "assert 'EmbarkedVec' in resultCols\n",
    "\n",
    "assert 'Sex' not in resultCols\n",
    "assert 'Civility' not in resultCols\n",
    "assert 'Embarked' not in resultCols\n",
    "\n",
    "assert result.schema['SexVec'].simpleString() == 'SexVec:vector'\n",
    "assert result.schema['CivilityVec'].simpleString() == 'CivilityVec:vector'\n",
    "assert result.schema['EmbarkedVec'].simpleString() == 'EmbarkedVec:vector'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dab91b6c7834a91ffce574703c292c6a",
     "grade": false,
     "grade_id": "cell-eea586c0506e7fed",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Question\n",
    "\n",
    "Now that we have created all of our numeric features, we need to assemble them into the same column. This is the goal of the [VectorAssembler](https://spark.apache.org/docs/2.2.0/ml-features.html#vectorassembler) transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.linalg import Vectors\n",
    "#from pyspark.ml.feature import VectorAssembler\n",
    "#df = data\n",
    "#assembler = VectorAssembler(inputCols=['Pclass', 'SibSp', 'Parch'], outputCol=\"features\")\n",
    "#result = assembler.setHandleInvalid(\"skip\").transform(df)\n",
    "#result.select('features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d779dc852a40f7ac15d24083cc86a5c",
     "grade": false,
     "grade_id": "cell-72faf34acdca3bb4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feature_assemble(df, featureCols):\n",
    "    \"\"\"\n",
    "    Assemble all features in the featureCols list into one column called 'features'.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "    result = assembler.setHandleInvalid(\"skip\").transform(df)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23b7efe52dcc5aeeac6114d27641f39c",
     "grade": true,
     "grade_id": "cell-7ad37566eead3ab5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graded cell\n",
    "\n",
    "2 points\n",
    "\"\"\"\n",
    "result = feature_assemble(data, ['Pclass', 'SibSp', 'Parch'])\n",
    "assert 'features' in result.columns\n",
    "assert result.schema['features'].simpleString() == 'features:vector'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c5df23e64ef240fbe5fb092a63cedf7",
     "grade": false,
     "grade_id": "cell-e0d94dd8381cf171",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "#### All the data preparation has been made. After running the following cell, we can concentrate on running ML modelling.\n",
    "\n",
    "For comparison purposes, let's try a Logistic Regression from MLlib and ML on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec8b4ddc9e87222209060ae5d0d05cdd",
     "grade": false,
     "grade_id": "cell-383133b054ab86e8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the data !\n",
    "features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'SexVec', 'CivilityVec', 'EmbarkedVec']\n",
    "prepared_data = feature_assemble(one_hot_encode(extract_civility(replace_na(data))), features)\n",
    "prepared_data = prepared_data.withColumnRenamed(\"Survived\", \"label\").select(['label', 'features'])\n",
    "train, test = prepared_data.randomSplit([0.75, 0.25], 0)\n",
    "\n",
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d6bb29333c6f86662eec9374a4ca529",
     "grade": false,
     "grade_id": "cell-e455c33086cadefb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## MLlib - RDD based API\n",
    "\n",
    "We will first use the RDD-based [Logistic Regression](https://spark.apache.org/docs/2.2.0/mllib-linear-methods.html#logistic-regression). The exercise comes into two steps :\n",
    "\n",
    "1. First, you must create a RDD of LabeledPoint(label, features). Also careful as we are using `pyspark.ml.linalg.SparseVector` but the RDD-based API expects `pyspark.mllib.linalg.SparseVector`, so we need to [convert it](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=linearregressionwithsgd#pyspark.mllib.linalg.Vectors.fromML).\n",
    "2. Then you can apply LogisticRegression on it.\n",
    "\n",
    "# Question\n",
    "\n",
    "Train a logistic regression model on the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.mllib.regression import LabeledPoint\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "#from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "#train_r= train.rdd.map(lambda row: LabeledPoint(row[0], Vectors.fromML(row[1])))\n",
    "#model = LogisticRegressionWithLBFGS.train(train_r, iterations=10, numClasses=2)\n",
    "#predictionAndLabels = test_rdd.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "#metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "#metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ea1f00a84a090feaff75579223363b3",
     "grade": false,
     "grade_id": "cell-3d344e0fc760f3e0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dataframe_to_labeledpoints(df):\n",
    "    \"\"\"\n",
    "    This function takes the conversion from a DataFrame of columns [label, features] to a RDD of LabeledPoint.    \n",
    "    \"\"\"\n",
    "    from pyspark.mllib.regression import LabeledPoint\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "    \n",
    "    return df.rdd.map(lambda row: LabeledPoint(row[0], Vectors.fromML(row[1])))\n",
    "\n",
    "def train_mllib_logistic(train):\n",
    "    \"\"\"\n",
    "    Return a MLlib logistic regression trained on a RDD of LabeledPoint. \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "    model = LogisticRegressionWithLBFGS.train(train, iterations=10, numClasses=2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a57929d658b4020e0f06871602d04b41",
     "grade": true,
     "grade_id": "cell-b54449b3f1087da2",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graded cell\n",
    "\n",
    "4 points\n",
    "\"\"\"\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "train_rdd = dataframe_to_labeledpoints(train)\n",
    "test_rdd = dataframe_to_labeledpoints(test)\n",
    "model = train_mllib_logistic(train_rdd)\n",
    "\n",
    "predictionAndLabels = test_rdd.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "assert metrics.areaUnderROC > 0.75  # I managed ~0.8 on my first try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c51bd15ed342f964c8419cce9892b37",
     "grade": false,
     "grade_id": "cell-0967807fd54e6521",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## ML - DataFrame based API\n",
    "\n",
    "We now compare with using the ML [Logistic regression](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#binomial-logistic-regression). It should work directly on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7922032693674484"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr=LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "model = lr.fit(train)\n",
    "model.summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8261937d04d9a450c5251a2ee9eed67d",
     "grade": false,
     "grade_id": "cell-50cb55db71381eac",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_ml_logistic(train):\n",
    "    \"\"\"\n",
    "    Return a MLlib logistic regression trained on a RDD of LabeledPoint. \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    lr=LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "    model = lr.fit(train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7def0348bf2f0ec5fa448f9093668879",
     "grade": true,
     "grade_id": "cell-2cb2116413925f78",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graded cell\n",
    "\n",
    "3 points\n",
    "\"\"\"\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "model = train_ml_logistic(train)\n",
    "assert model.summary.areaUnderROC > 0.75 # managed 0.87 on my first try\n",
    "\n",
    "predictions = model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "assert evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"}) > 0.75 # managed 0.88 on my first try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce916c26c6d960dbca976554f0a31bba",
     "grade": false,
     "grade_id": "cell-cc43fdd28fe66f9b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
